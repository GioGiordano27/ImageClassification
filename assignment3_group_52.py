# -*- coding: utf-8 -*-
"""assignment3_group-52.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_7UxvRzeQ-DxE3FZdq12N_c_1PAHYX8h
"""

import numpy as np
import math
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Flatten, Dense
from keras import layers
from typing import List
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
from copy import deepcopy

# !pip install torchmetrics
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

"""# Load dataset"""

(x_train_initial, y_train), (x_test_initial, y_test) = tf.keras.datasets.fashion_mnist.load_data()
plt.imshow(x_train_initial[0]) #show first imagine
y_train[0] #class label for image above

"""# Normalize dataset"""

x_train_unnormalized = x_train_initial.reshape(60000, 28**2)
x_test_unnormalized = x_test_initial.reshape(10000, 28**2)

x_train = (x_train_unnormalized - np.mean(x_train_unnormalized, axis=0))/np.std(x_train_unnormalized, axis=0)
x_test = (x_test_unnormalized - np.mean(x_test_unnormalized, axis=0))/np.std(x_test_unnormalized, axis=0)

x_train_u, x_valid_u, y_train_u, y_valid_u = train_test_split(x_train_unnormalized, y_train, train_size=0.8) #default shuffle=True
x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=0.8) #default shuffle=True
y_train_conv, y_valid_conv, y_test_conv = y_train, y_valid, y_test

print(type(x_train))
print(x_train.shape)

"""# Hot One Encode labels"""

y_train = np.eye(10)[y_train]
y_valid = np.eye(10)[y_valid]
y_test = np.eye(10)[y_test]
y_train_u = np.eye(10)[y_train_u]
y_valid_u = np.eye(10)[y_valid_u]


print(y_train.shape)

"""# Neural Network

## Neural Net Layer
"""

class NeuralNetLayer:
    def __init__(self):
        self.gradient = None
        self.parameters = None

    def forward(self, x):
        raise NotImplementedError

    def backward(self, gradient):
        raise NotImplementedError

"""## Linear Layer"""

class LinearLayer(NeuralNetLayer):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.ni = input_size
        self.no = output_size
        self.w = 0.01 * np.random.randn(output_size, input_size)
        self.b = 0.01 * np.random.randn(output_size)

        self.cur_input = None
        self.parameters = [self.w, self.b]

    def forward(self, x):
        self.cur_input = x
        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b

    def backward(self, gradient):
        assert self.cur_input is not None, "Must call forward before backward"
        dw = gradient[:, :, None] @ self.cur_input[:, None, :]
        db = gradient
        self.gradient = [dw, db]
        return gradient.dot(self.w)

# This is for higher layers

class LinearLayerX(NeuralNetLayer):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.ni = input_size
        self.no = output_size

        self.w = 0.1 * np.random.randn(output_size, input_size)
        self.b = 0.1 * np.random.randn(output_size)

        self.cur_input = None
        self.parameters = [self.w, self.b]

    def forward(self, x):
        self.cur_input = x
        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b

    def backward(self, gradient):
        assert self.cur_input is not None, "Must call forward before backward"
        dw = gradient[:, :, None] @ self.cur_input[:, None, :]
        db = gradient
        self.gradient = [dw, db]
        return gradient.dot(self.w)

"""## Relu Layer"""

class ReLULayer(NeuralNetLayer):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        self.gradient = np.where(x > 0, 1.0, 0.0)
        return np.maximum(0, x)

    def backward(self, gradient):
        assert self.gradient is not None, "Must call forward before backward"
        return gradient * self.gradient

"""## Softmax"""

class SoftmaxOutputLayer(NeuralNetLayer):
    def __init__(self):
        super().__init__()
        self.cur_probs = None

    def forward(self, x):
        x = x - np.max(x, axis=-1, keepdims=True)
        exps = np.exp(x)
        probs = exps / np.sum(exps, axis=-1)[:, None]
        self.cur_probs = probs
        return probs

    def backward(self, target):
        assert self.cur_probs is not None, "Must call forward before backward"
        return self.cur_probs - target

"""## Tanh"""

class tanhLayer(NeuralNetLayer):
  def __init__(self):
        super().__init__()

  def forward(self, x):
    self.gradient = 1 - (np.tanh(x))**2
    return np.tanh(x)

  def backward(self, gradient):
    assert self.gradient is not None, "Must call forward before backward"
    return gradient * self.gradient

"""## Leaky-ReLU"""

class LeakyReLULayer(NeuralNetLayer):
    def __init__(self, slope):
        super().__init__()
        self.slope = slope # For the negative part

    def forward(self, x):
        self.gradient = np.where(x > 0, 1.0, self.slope)
        return np.maximum(0, x) + self.slope * np.minimum(0,x)

    def backward(self, gradient):
        assert self.gradient is not None, "Must call forward before backward"
        return gradient * self.gradient

"""## Optimizer"""

class Optimizer:
    def __init__(self, net):
        self.net = net

    def step(self):
        for layer in self.net.layers[::-1]:
            if layer.parameters is not None:
                self.update(layer.parameters, layer.gradient)

    def update(self, params, gradient):
        raise NotImplementedError()

class GradientDescentOptimizer(Optimizer):
    def __init__(self, net, lr: float):
        super().__init__(net)
        self.lr = lr

    def update(self, params, gradient):
        for (p, g) in zip(params, gradient):
            p -= self.lr * g.mean(axis=0)

"""## Useful Functions"""

def ce(yhat, y, layers, alpha=0, l2=False):
    if l2:
      l2_norm = 0
      for layer in layers:
        if layer.parameters is not None:
          l2_norm += layer.w ** 2

      return -1 * np.sum(y * np.log(yhat + 1e-100))/len(y) + alpha * l2_norm

    else:
      return -1 * np.sum(y * np.log(yhat + 1e-100))/len(y)


def evaluate_acc(yhat, y): #they are one hot encoded
    yhat = np.argmax(yhat, axis=-1)
    y = np.argmax(y, axis=-1)
    return accuracy_score(yhat, y)

def ce_vs_epoch(losses, losses_valid=None, name=None, title=None):
    plt.clf()
    plt.plot(losses, label = "Train")
    if losses_valid is not None:
        plt.plot(losses_valid, label = "Valid")
    plt.xlabel("Epoch")
    plt.ylabel("CE loss")
    plt.title(f"CE vs Epoch: {title}")
    plt.legend(loc='top right')
    plt.savefig(f"training_vs_valid_CE_{name}.png", bbox_inches="tight", dpi=300)
    plt.show()

def acc_vs_epoch(acc, acc_valid=None, name=None, title=None):
    plt.clf()
    plt.plot(acc, label = "Train")
    if acc_valid is not None:
        plt.plot(acc_valid, label = "Valid")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(loc='lower right')
    plt.title(f"Accuracy vs Epoch: {title}")
    plt.savefig(f"training_vs_valid_acc_{name}.png", bbox_inches="tight", dpi=300)
    plt.show()

"""## MLP"""

class MLP:
    def __init__(self, *args: List[NeuralNetLayer]):
        self.layers = args
        self.best = None

    def forward(self, x):
        for layer in self.layers:
            x = layer.forward(x)
        self.out = x
        return x

    def backward(self, target):
        for layer in self.layers[::-1]:
            target = layer.backward(target)

    def fit(self, optimizer, x_train, y_train, x_valid, y_valid, alpha=0, l2 = False, steps = 50, batch_size = 100):
        losses, losses_valid, acc_train, acc_valid = [], [], [], []
        lowest_ce = math.inf

        for iteration in tqdm(range(steps)):
            batch_loss = []
            batch_accuracy = []
            for i in range(0, x_train.shape[0], batch_size):
                batch_x = x_train[i:i+batch_size]
                labels_y = y_train[i:i+batch_size]
                yhat = self.predict(batch_x) #default testing=False
                b_loss = ce(yhat, labels_y, self.layers, l2)
                b_accuracy = evaluate_acc(yhat, labels_y)
                self.backward(labels_y)
                optimizer.step()
                batch_loss.append(b_loss)
                batch_accuracy.append(b_accuracy)

            #average training loss and accuracy
            loss = sum(batch_loss)/len(batch_loss)
            losses.append(loss)
            accuracy_train = sum(batch_accuracy)/len(batch_accuracy)
            acc_train.append(accuracy_train)

            #valid set
            predictions_valid = self.predict(x_valid)
            loss_valid = ce(predictions_valid, y_valid, self.layers, l2)
            losses_valid.append(loss_valid)
            accuracy_valid = evaluate_acc(predictions_valid, y_valid)
            acc_valid.append(accuracy_valid)

            if loss_valid < lowest_ce:
                lowest_ce = loss_valid
                self.best = deepcopy(self.layers)

            print("iteration {0}: train loss {1}, train acc {2}, valid loss {3}, valid acc {4} ".format(iteration, loss, accuracy_train, loss_valid, accuracy_valid))
        return self, losses, losses_valid, acc_train, acc_valid

    def predict(self, x_test, testing=False):
        if testing:
            for layer in self.best:
                x_test = layer.forward(x_test)
            return x_test
        return self.forward(x_test)

    def check_gradient(self, x, y):
        grads_w, grads_b = [], []
        for layer in self.layers:
            if not isinstance(layer, LinearLayer): #need layer.no and layer.ni
                continue
            eps_w = np.random.rand(layer.no, layer.ni) * 1e-19
            eps_b = np.random.rand(layer.no) * 1e-16

            #w - eps
            layer.w -= eps_w
            yhat = self.forward(x)
            ce1 = ce(y = y, yhat = yhat, layers = self.layers)

            #w + eps
            layer.w += 2*eps_w
            yhat = self.forward(x)
            ce2 = ce(y = y, yhat = yhat, layers = self.layers)
            layer.w -= eps_w #reset weights

            dw_num = (ce1 - ce2)/(2*eps_w) # approximated gradient
            dw_cal = layer.gradient[0].mean(axis=0)
            grad_diff = (dw_cal - dw_num)**2
            grad_diff = grad_diff/(dw_cal + dw_num)**2
            grads_w.append(eps_w.mean())
            grads_b.append(eps_b.mean())
        s = "Weight gradient difference: {}\n".format(grads_w)
        s += "Biases gradient difference: {}".format(grads_b)
        print(s)
        return grads_w, grads_b

"""# Task 3.1

Initialize features
"""

HIDDEN_SIZE = 128
GRADIENT_STEPS = 15
BATCH_SIZE = 100
LEAKY_SLOPE = 1e-2
LEARNING_RATE = 1e-2

acc_train_layers = []
acc_test_layers = []

train_acc_layers = []
valid_test_acc_layers = []

"""## 0 Layer MLP"""

mlp0 = MLP(LinearLayer(28**2, 10),
           SoftmaxOutputLayer()
)

opt0 = GradientDescentOptimizer(mlp0, LEARNING_RATE)
mlp, losses_train, losses_valid, acc_train, acc_valid = mlp0.fit(opt0, x_train, y_train, x_valid, y_valid, steps=GRADIENT_STEPS, batch_size=BATCH_SIZE)

train_acc_layers.append(acc_train)
valid_test_acc_layers.append(acc_valid)

print(losses_train)
ce_vs_epoch(losses_train, losses_valid, "0_layer", "0 Layer")
acc_vs_epoch(acc_train, acc_valid, "0_layer", "0 Layer")

"""### 0 Layer Training Accuracy"""

predictions = mlp.predict(x_train, testing=True)
acc = evaluate_acc(predictions, y_train)
acc_train_layers.append(acc)
print("Training accuracy with 0 layer mlp: ", acc)

"""### 0 Layer Testing Accuracy"""

predictions = mlp.predict(x_test, testing=True)
acc = evaluate_acc(predictions, y_test)
acc_test_layers.append(acc)
print("Testing accuracy with 0 mlp: ", acc)

"""### Check Gradient"""

grads = mlp.check_gradient(x_train, y_train)

"""## 1 Layer MLP"""

mlp1 = MLP(
    LinearLayer(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayer(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt1 = GradientDescentOptimizer(mlp1, LEARNING_RATE)

mlp, losses_train, losses_valid, acc_train, acc_valid = mlp1.fit(opt1, x_train, y_train, x_valid, y_valid, steps=GRADIENT_STEPS, batch_size = BATCH_SIZE)

train_acc_layers.append(acc_train)
valid_test_acc_layers.append(acc_valid)

ce_vs_epoch(losses_train, losses_valid, "1_layer", "1 Layer")
acc_vs_epoch(acc_train, acc_valid, "1_layer", "1 Layer")

"""### Training Accuracy"""

predictions = mlp.predict(x_train, testing=True)
acc = evaluate_acc(predictions, y_train)
acc_train_layers.append(acc)
print("Training accuracy with 1 layer mlp: ", acc)

"""### Testing Accuracy"""

predictions = mlp.predict(x_test, testing=True)
acc = evaluate_acc(predictions, y_test)
acc_test_layers.append(acc)
print("Testing accuracy with 1 layer mlp: ", acc)

"""### Check Gradient"""

grads = mlp.check_gradient(x_train, y_train)

"""## 2 Layer MLP"""

mlp2 = MLP(
    LinearLayer(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayer(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt2 = GradientDescentOptimizer(mlp2, LEARNING_RATE)

mlp, losses_train, losses_valid, acc_train, acc_valid = mlp2.fit(opt2, x_train, y_train, x_valid, y_valid, steps = GRADIENT_STEPS, batch_size=BATCH_SIZE)

train_acc_layers.append(acc_train)
valid_test_acc_layers.append(acc_valid)

ce_vs_epoch(losses_train, losses_valid, "2_layer", "2 Layer")
acc_vs_epoch(acc_train, acc_valid, "2_layer", "2 Layer")

"""### Training Accuracy"""

predictions = mlp.predict(x_train, testing=True)
acc = evaluate_acc(predictions, y_train)
acc_train_layers.append(acc)
print("Training Accuracy with 2 layer MLP: ", acc)

"""### Testing Accuracy"""

predictions = mlp.predict(x_test, testing=True)
acc = evaluate_acc(predictions, y_test)
acc_test_layers.append(acc)
print("Testing Accuracy with 2 layer MLP: ", acc)

"""### Check Gradient"""

grads = mlp.check_gradient(x_train, y_train)

# import tensorflow as tf
#keras.model.Se
(x_train1, y_train1), _ = tf.keras.datasets.fashion_mnist.load_data()

model = Sequential(
    [
        Flatten(input_shape=(28, 28)),
        Dense(128, activation=tf.nn.relu),
        Dense(128, activation=tf.nn.relu),
        Dense(10, activation=tf.nn.softmax),
    ]
)

model.summary()

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.fit(x_train1, y_train1, epochs=GRADIENT_STEPS)
loss, accuracy = model.evaluate(x_train1, y_train1)
print("Train accuracy:", accuracy)

"""# Task 3.2

## Tanh
"""

mlp_tanh = MLP(
    LinearLayer(28**2, HIDDEN_SIZE),
    tanhLayer(),
    LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE),
    tanhLayer(),
    LinearLayer(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt_tanh = GradientDescentOptimizer(mlp_tanh, LEARNING_RATE)
mlp, losses_train, losses_valid, acc_train, acc_valid = mlp_tanh.fit(opt_tanh, x_train, y_train, x_valid, y_valid, steps = GRADIENT_STEPS, batch_size=BATCH_SIZE)

ce_vs_epoch(losses_train, losses_valid, "tanh", "Tanh")
acc_vs_epoch(acc_train, acc_valid, "tanh", "Tanh")

"""### Training Accuracy Tanh"""

predictions = mlp.predict(x_train)
acc = evaluate_acc(predictions, y_train)
print("Training Accuracy of 2 layer MLP Tanh: ", acc)

"""### Testing Accuracy Tanh"""

predictions = mlp.predict(x_test)
acc = evaluate_acc(predictions, y_test)
print("Test Accuracy of 2 layer MLP Tanh: ", acc)

"""### Check Gradient"""

grads = mlp.check_gradient(x_train, y_train)

"""## Leaky-Relu"""

mlp_leaky = MLP(
    LinearLayer(28**2, HIDDEN_SIZE),
    LeakyReLULayer(slope = LEAKY_SLOPE),
    LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE),
    LeakyReLULayer(slope = LEAKY_SLOPE),
    LinearLayer(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt_leaky = GradientDescentOptimizer(mlp_leaky, LEARNING_RATE)
mlp, losses_train, losses_valid, acc_train, acc_valid = mlp_leaky.fit(opt_leaky, x_train, y_train,
                   x_valid, y_valid, steps = GRADIENT_STEPS, batch_size=BATCH_SIZE)

ce_vs_epoch(losses_train, losses_valid, "leaky_relu", "Leaky-Relu")
acc_vs_epoch(acc_train, acc_valid, "leaky_relu", "Leaky-Relu")

"""### Training Accuracy Leaky-Relu"""

predictions = mlp.predict(x_train)
acc = evaluate_acc(predictions, y_train)
print("Training Accuracy of 2 layer MLP Leaky: ", acc)

"""### Testing Accuracy Leaky-Relu"""

predictions = mlp.predict(x_test)
acc = evaluate_acc(predictions, y_test)
print("Testing Accuracy of 2 layer MLP Leaky: ", acc)

"""### Check Gradient"""

grads = mlp.check_gradient(x_train, y_train)

"""## Conclusions of different activation functions

From our tests, we can see that all three models actually give similar accuracies (approx. 86%) when trained with a good amount of epochs. However, the MLP with Relu activation still performed best compared to the MLP with tanh and leaky-rely activations.

# Task 3.3 (L2-Regularization)
"""

lambdas = [0, 0.1, 0.5, 1, 5, 10]
l2_train_acc = []
l2_test_acc = []

for l in lambdas:
    mlp_l2 = MLP(
        LinearLayer(28**2, HIDDEN_SIZE),
        ReLULayer(),
        LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE),
        ReLULayer(),
        LinearLayer(HIDDEN_SIZE, 10),
        SoftmaxOutputLayer()
    )
    opt_l2 = GradientDescentOptimizer(mlp_l2, LEARNING_RATE)

    mlp_l2, losses_train, losses_valid, acc_train, acc_valid = mlp_l2.fit(opt_l2, x_train, y_train, x_valid, y_valid, alpha=l, l2=True, steps = GRADIENT_STEPS, batch_size = BATCH_SIZE)

    # Training acc
    predictions = mlp_l2.predict(x_train, testing=True)
    l2_train_acc.append(acc_train)

    # Testing acc
    predictions = mlp_l2.predict(x_test, testing=True)
    l2_test_acc.append(acc_valid)


# Plot training accuracy over different lambdas
plt.clf()
plt.plot(l2_train_acc[0], label = "λ = 0")
plt.plot(l2_train_acc[1], label = "λ = 0.1")
plt.plot(l2_train_acc[2], label = "λ = 0.5")
plt.plot(l2_train_acc[3], label = "λ = 1")
plt.plot(l2_train_acc[4], label = "λ = 5")
plt.plot(l2_train_acc[5], label = "λ = 10")
plt.xlabel("Epoch")
plt.ylabel("L2-Accuracy")
plt.legend(loc='lower right')
plt.title(f"L2-Accuracy vs Epoch: Training Set")
plt.savefig(f"training_vs_valid_acc_diff_lambdas_train.png", bbox_inches="tight", dpi=300)
plt.show()

# Plot testing accuracy over different lambdas
plt.clf()
plt.plot(l2_test_acc[0], label = "λ = 0")
plt.plot(l2_test_acc[1], label = "λ = 0.1")
plt.plot(l2_test_acc[2], label = "λ = 0.5")
plt.plot(l2_test_acc[3], label = "λ = 1")
plt.plot(l2_test_acc[4], label = "λ = 5")
plt.plot(l2_test_acc[5], label = "λ = 10")
plt.xlabel("Epoch")
plt.ylabel("L2-Accuracy")
plt.legend(loc='lower right')
plt.title(f"L2-Accuracy vs Epoch: Testing Set")
plt.savefig(f"training_vs_valid_acc_diff_lambdas_test.png", bbox_inches="tight", dpi=300)
plt.show()

"""# Task 3.4"""

print(x_train_u.shape)
print(x_train.shape)

mlp_unnormalized = MLP(LinearLayer(28**2, HIDDEN_SIZE),
                       ReLULayer(),
                       LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE),
                       ReLULayer(),
                       LinearLayer(HIDDEN_SIZE, 10),
                       SoftmaxOutputLayer())
opt_unnormalized = GradientDescentOptimizer(mlp_unnormalized, LEARNING_RATE)

mlp = mlp_unnormalized.fit(opt_unnormalized, x_train_u, y_train_u, x_valid_u, y_valid_u, steps = GRADIENT_STEPS, batch_size = BATCH_SIZE)[0]

"""## Training Accuracy Unnormalized"""

predictions = mlp.predict(x_train_u, testing=True)
acc = evaluate_acc(predictions, y_train_u)
print("Training Accuracy with 2 layer MLP Unnormalized: ", acc)

"""## Testing Accuracy Unnormalized"""

predictions = mlp.predict(x_test_unnormalized, testing=True)
acc = evaluate_acc(predictions, y_test)
print("Testing Accuracy with 2 layer MLP Unnormalized: ", acc)

"""### Check Gradient"""

grads = mlp.check_gradient(x_train, y_train)

"""## Conclusions Normalized vs Unnormalized

From our tests, it seems like normalizing the dataset does not provide a greater testing accuracy compared to the model trained with unnormalized data. However, this is mainly a result of a very small variance when initializing the weights and biaises in the LinearLayer.

# Task 3.5

## Convolutional Neural Network
"""

# Base model ( overfits )
model = Sequential()
model.add(layers.Conv2D(16, kernel_size=(3,3),
                        activation="relu",
                        input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(16,
                        kernel_size=(3,3),
                        activation="relu",
                        ))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation="relu"))
model.add(Dense(128, activation="relu"))
model.add(Dense(10, activation="softmax"))
model.summary()

print(x_train.reshape(x_train.shape[0], 28, 28, 1).shape)

num_epoch = 15
model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

plt.plot(trained.history['accuracy'], label='accuracy')
plt.plot(trained.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

# score = model.evaluate(x_test.reshape(x_test.shape[0], 28, 28, 1), y_test, verbose=False) #returns list
# print("Trained with {} epochs: ".format(num_epoch))
# print('Test loss:', score[0])
# print('Test accuracy:', score[1])

"""### Fitting hyperparameters

#### Kernal size
"""

N = 4
models = [0] * N
print(models)
for model_num in range(N):
  models[model_num] = Sequential()
  models[model_num].add(layers.Conv2D(16, kernel_size=(model_num+2,model_num+2),
                        activation="relu", input_shape=(28, 28, 1)))
  models[model_num].add(layers.MaxPooling2D((2,2)))
  models[model_num].add(layers.Conv2D(16,
                        kernel_size=(model_num+2,model_num+2),
                        activation="relu",
                        ))
  models[model_num].add(layers.MaxPooling2D(pool_size=(2, 2)))
  models[model_num].add(Flatten())
  models[model_num].add(Dense(128, activation="relu"))
  models[model_num].add(Dense(128, activation="relu"))
  models[model_num].add(Dense(10, activation="softmax"))
  models[model_num].summary()

num_epoch = 15
filter_size = 0
for model in models:
  model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
  trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

  filter_size += 2
  plt.plot(trained.history['accuracy'], label="Filter size " + str(filter_size))
  #plt.plot(trained.history['val_accuracy'], label = "Filter_size " + str(filter_size))
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0.5, 1])
  plt.legend(loc='lower right')

"""#### Stride length"""

N = 4
models = [0] * N
for model_num in range(N):
  models[model_num] = Sequential()
  models[model_num].add(layers.Conv2D(16, kernel_size=(2,2), activation="relu", input_shape=(28, 28, 1),
                                      strides = (model_num +1 , model_num +1),  padding = 'same'))
  models[model_num].add(layers.MaxPooling2D((2,2),padding='same'))
  models[model_num].add(layers.Conv2D(16, kernel_size=(2,2), activation="relu",
                                      strides = (model_num + 1, model_num + 1), padding = 'same'))
  models[model_num].add(layers.MaxPooling2D(pool_size=(2, 2),padding='same'))
  models[model_num].add(Flatten())
  models[model_num].add(Dense(128, activation="relu"))
  models[model_num].add(Dense(128, activation="relu"))
  models[model_num].add(Dense(10, activation="softmax"))
  models[model_num].summary()

num_epoch = 15
stride_size = 0
for model in models:
  model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
  trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

  stride_size += 1
  plt.plot(trained.history['accuracy'], label="Stride size " + str(stride_size))
  #plt.plot(trained.history['val_accuracy'], label = "Stride size " + str(stride_size))
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0.5, 1])
  plt.legend(loc='lower right')

"""#### Max pooling window size"""

N = 4
models = [0] * N
for model_num in range(N):
  models[model_num] = Sequential()
  models[model_num].add(layers.Conv2D(16, kernel_size=(2,2), activation="relu", input_shape=(28, 28, 1),
                                      strides = (1,1),  padding = 'same'))
  models[model_num].add(layers.MaxPooling2D((model_num + 2, model_num + 2)))
  models[model_num].add(layers.Conv2D(16, kernel_size=(2,2), activation="relu",
                                      strides = (1,1), padding = 'same'))
  models[model_num].add(layers.MaxPooling2D(pool_size=(model_num + 2, model_num + 2), strides = (1,1)))
  models[model_num].add(Flatten())
  models[model_num].add(Dense(128, activation="relu"))
  models[model_num].add(Dense(128, activation="relu"))
  models[model_num].add(Dense(10, activation="softmax"))
  models[model_num].summary()

num_epoch = 15
pooling_window_size = 1
for model in models:
  model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
  trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

  pooling_window_size += 1
  plt.plot(trained.history['accuracy'], label="Window size " + str(pooling_window_size))
  #plt.plot(trained.history['val_accuracy'], label = "Window size " + str(pooling_window_size))
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0.5, 1])
  plt.legend(loc='lower right')

"""### Dropout models and fitting dropout rate"""

# Model 1 ( only apply small dropout on the convolution layers )
dropout_rate = [0, 0.0625, 0.125, 0.1875, 0.25]
N = 5
models = [0] * 5
for i in range(len(models)):
  model = Sequential()
  model.add(layers.Conv2D(16, kernel_size=(3,3),
                        activation="relu",
                        input_shape=(28, 28, 1)))
  model.add(layers.MaxPooling2D((2,2)))
  model.add(layers.Dropout(dropout_rate[i]))
  model.add(layers.Conv2D(16,
                        kernel_size=(3,3),
                        activation="relu",
                        ))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(dropout_rate[i]))
  model.add(Flatten())
  model.add(Dense(128, activation="relu"))
  model.add(Dense(128, activation="relu"))
  model.add(Dense(10, activation="softmax"))
  #model.summary()
  models[i] = model

num_epoch = 15
drop_rate = 0
for model in models:
  model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
  trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

  #plt.plot(trained.history['accuracy'], label="Dropout rate " + str(drop_rate))
  plt.plot(trained.history['val_accuracy'], label = "Dropout rate " + str(drop_rate))
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0.5, 1])
  plt.legend(loc='lower right')
  drop_rate += 0.0625

# Model 2 ( only apply dropout on the dense layers )
dropout_rate = [0, 0.25, 0.5]
N = 3
models = [0] * 3
for i in range(len(models)):
  model = Sequential()
  model.add(layers.Conv2D(16, kernel_size=(3,3),
                        activation="relu",
                        input_shape=(28, 28, 1)))
  model.add(layers.MaxPooling2D((2,2)))
  model.add(layers.Conv2D(16,
                        kernel_size=(3,3),
                        activation="relu",
                        ))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(Flatten())
  model.add(Dense(128, activation="relu"))
  model.add(layers.Dropout(dropout_rate[i]))
  model.add(Dense(128, activation="relu"))
  model.add(layers.Dropout(dropout_rate[i]))
  model.add(Dense(10, activation="softmax"))
  #model.summary()
  models[i] = model

num_epoch = 2
drop_rate = 0
for model in models:
  model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
  trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

  #plt.plot(trained.history['accuracy'], label="Dropout rate " + str(drop_rate))
  plt.plot(trained.history['val_accuracy'], label = "Dropout rate " + str(drop_rate))
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0.5, 1])
  plt.legend(loc='lower right')
  drop_rate += 0.25

# Mix of model 1 and model 2
dropout_rate_conv = [0, 0.10, 0.2]
dropout_rate_dense = [0, 0.25, 0.5]
N = 3
models = [0] * 3
for i in range(len(models)):
  model = Sequential()
  model.add(layers.Conv2D(16, kernel_size=(3,3),
                        activation="relu",
                        input_shape=(28, 28, 1)))
 #model.add(layers.Dropout(dropout_rate_conv[i]))
  model.add(layers.MaxPooling2D((2,2)))
  model.add(layers.Dropout(dropout_rate_conv[i]))
  model.add(layers.Conv2D(16,
                        kernel_size=(3,3),
                        activation="relu",
                        ))
 #model.add(layers.Dropout(dropout_rate_conv[i]))
  model.add(layers.MaxPooling2D(pool_size=(2, 2)))
  model.add(layers.Dropout(dropout_rate_conv[i]))
  model.add(Flatten())
  model.add(Dense(128, activation="relu"))
  model.add(layers.Dropout(dropout_rate_dense[i]))
  model.add(Dense(128, activation="relu"))
  model.add(layers.Dropout(dropout_rate_dense[i]))
  model.add(Dense(10, activation="softmax"))
  #model.summary()
  models[i] = model

num_epoch = 15
drop_rate = [0,0]
for model in models:
  model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
  trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

  #plt.plot(trained.history['accuracy'], label="Dropout rate " + str(drop_rate))
  plt.plot(trained.history['val_accuracy'], label = "Dropout rate " + str(drop_rate))
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0.5, 1])
  plt.legend(loc='lower right')
  drop_rate[0] += 0.1
  drop_rate[1] += 0.25

"""### Best Convolutional Model"""

model = Sequential()
model.add(layers.Conv2D(16, kernel_size=(2,2), activation="relu", input_shape=(28, 28, 1),
                        strides = (1,1),  padding = 'same'))
model.add(layers.MaxPooling2D((2,2)))
model.add(layers.Conv2D(16, kernel_size=(2,2), activation="relu",
                        strides = (1,1),  padding = 'same'))
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation="relu"))
model.add(layers.Dropout(0.25))
model.add(Dense(128, activation="relu"))
model.add(layers.Dropout(0.25))
model.add(Dense(10, activation="softmax"))
model.summary()

num_epoch = 15
model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=["accuracy"])
trained = model.fit(x_train.reshape(x_train.shape[0], 28, 28, 1), y_train_conv, epochs=num_epoch,
                    validation_data = (x_test.reshape(x_test.shape[0], 28, 28, 1), y_test_conv))

plt.plot(trained.history['accuracy'], label='accuracy')
plt.plot(trained.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

"""# Task 3.6

## Increasing the amount of hidden layers (3 layers)
"""

mlp3Layer = MLP(
    LinearLayerX(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt3 = GradientDescentOptimizer(mlp3Layer, LEARNING_RATE)

mlp4Layer = MLP(
    LinearLayerX(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt4 = GradientDescentOptimizer(mlp4Layer, LEARNING_RATE)

mlp5Layer = MLP(
    LinearLayerX(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt5 = GradientDescentOptimizer(mlp5Layer, LEARNING_RATE)

mlp3, losses_train, losses_valid, acc_train, acc_valid = mlp3Layer.fit(opt3, x_train, y_train, x_valid, y_valid, steps=GRADIENT_STEPS, batch_size=BATCH_SIZE)
grads = mlp3.check_gradient(x_train, y_train)
train_acc_layers.append(acc_train)
valid_test_acc_layers.append(acc_valid)

mlp4, losses_train, losses_valid, acc_train, acc_valid = mlp4Layer.fit(opt4, x_train, y_train, x_valid, y_valid, steps=GRADIENT_STEPS, batch_size=BATCH_SIZE)
grads = mlp4.check_gradient(x_train, y_train)
train_acc_layers.append(acc_train)
valid_test_acc_layers.append(acc_valid)

mlp5, losses_train, losses_valid, acc_train, acc_valid = mlp5Layer.fit(opt5, x_train, y_train, x_valid, y_valid, steps=GRADIENT_STEPS, batch_size=BATCH_SIZE)
grads = mlp5.check_gradient(x_train, y_train)
train_acc_layers.append(acc_train)
valid_test_acc_layers.append(acc_valid)


# Plot training accuracy over 0 to 4 hidden layers
plt.clf()
plt.plot(train_acc_layers[0], label = "0-layer")
plt.plot(train_acc_layers[1], label = "1-layer")
plt.plot(train_acc_layers[2], label = "2-layer")
plt.plot(train_acc_layers[3], label = "3-layer")
plt.plot(train_acc_layers[4], label = "4-layer")
plt.plot(train_acc_layers[5], label = "5-layer")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc='lower right')
plt.title(f"Accuracy vs Epoch: Training Set")
plt.savefig(f"training_vs_valid_acc_diff_layers_train.png", bbox_inches="tight", dpi=300)
plt.show()

# Plot testing accuracy over 0 to 4 hidden layers
plt.clf()
plt.plot(valid_test_acc_layers[0], label = "0-layer")
plt.plot(valid_test_acc_layers[1], label = "1-layer")
plt.plot(valid_test_acc_layers[2], label = "2-layer")
plt.plot(valid_test_acc_layers[3], label = "3-layer")
plt.plot(valid_test_acc_layers[4], label = "4-layer")
plt.plot(valid_test_acc_layers[5], label = "5-layer")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc='lower right')
plt.title(f"Accuracy vs Epoch: Testing Set")
plt.savefig(f"training_vs_valid_acc_diff_layers_test.png", bbox_inches="tight", dpi=300)
plt.show()

"""## Changing the width of hidden layers as we go deeper"""

# HIDDEN_SIZE = 128
mlp2Layer = MLP(
    LinearLayer(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayer(HIDDEN_SIZE, HIDDEN_SIZE//2),
    ReLULayer(),
    LinearLayer(HIDDEN_SIZE//2, 10),
    SoftmaxOutputLayer()
)
opt2 = GradientDescentOptimizer(mlp2Layer, LEARNING_RATE)


mlp3Layer = MLP(
    LinearLayerX(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE, HIDDEN_SIZE//2),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE//2, HIDDEN_SIZE//4),
    ReLULayer(),
    LinearLayerX(HIDDEN_SIZE//4, 10),
    SoftmaxOutputLayer()
)
opt3 = GradientDescentOptimizer(mlp3Layer, LEARNING_RATE)



mlp2, losses_train, losses_valid, acc_train, acc_valid = mlp2Layer.fit(opt2, x_train, y_train, x_valid, y_valid, steps=15, batch_size = BATCH_SIZE)
grads = mlp2.check_gradient(x_train, y_train)
plt.clf()
plt.plot(acc_train, label = "Train")
plt.plot(acc_valid, label = "Test")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc='lower right')
plt.title("Accuracy vs Epoch: Width size reduction on 2 layers")
plt.savefig("training_vs_valid_acc_width_reduction_2_layers.png", bbox_inches="tight", dpi=300)
plt.show()

mlp3, losses_train, losses_valid, acc_train, acc_valid = mlp3Layer.fit(opt3, x_train, y_train, x_valid, y_valid, steps=15, batch_size = BATCH_SIZE)
grads = mlp3.check_gradient(x_train, y_train)
plt.clf()
plt.plot(acc_train, label = "Train")
plt.plot(acc_valid, label = "Test")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc='lower right')
plt.title("Accuracy vs Epoch: Width size reduction on 3 layers")
plt.savefig("training_vs_valid_acc_width_reduction_3_layers.png", bbox_inches="tight", dpi=300)
plt.show()

"""## Our MLP model with the best parameters

Number of hidden layers: 1

Activation Function: ReLU

L2-Regularization: No

Number of epochs: 15

Normalize data: No

Width reduction: No
"""

mlp1 = MLP(
    LinearLayer(28**2, HIDDEN_SIZE),
    ReLULayer(),
    LinearLayer(HIDDEN_SIZE, 10),
    SoftmaxOutputLayer()
)
opt1 = GradientDescentOptimizer(mlp1, LEARNING_RATE)

mlp, losses_train, losses_valid, acc_train, acc_valid = mlp1.fit(opt1, x_train, y_train, x_valid, y_valid, steps=15, batch_size = BATCH_SIZE)
grads = mlp.check_gradient(x_train, y_train)

plt.clf()
plt.plot(acc_train, label = "Train")
plt.plot(acc_valid, label = "Test")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend(loc='lower right')
plt.title("Accuracy vs Epoch: Best parameters")
plt.savefig("training_vs_valid_acc_best_params.png", bbox_inches="tight", dpi=300)
plt.show()